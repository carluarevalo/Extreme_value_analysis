---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.4.2
  kernelspec:
    display_name: R
    language: R
    name: ir
output:
  html_document:
    df_print: paged
---

Escribo ac치 algunos posibles ejercicios

$$\huge 游뗻$$


En esta gu칤a veremos c칩mo modelar una serie de datos observados usando:

1. **M치ximos por Per칤odos** y el ***Teorema de Generalized Extreme Values (GEV)***

2. **Excesos sobre Umbral** y el ***Teorema de Generalized Pareto Distribution (GPD)***

Los datos se corresponden al dataset de $\mathcal C$arli  `[y ac치 ir칤a descripci칩n del dataset]`.

1. Para m치ximos por per칤odos: 

> usamos Gumbell como dijo Mariela? justificando que es el caso m치s com칰n y (tal vez) simple de analizar.
>
> Mencionamos usos de los otros dos casos? maybe grafiquitos en imagen y fin?

1. Para umbral

> Hay solo una, pero con par치metro = 0

3. Para ambos

> * Filtrar datos => Ajustar F => Devolver par치metros
> Maybe: Guardar matriz de var-cov => calcular varianza del nivel de retorno $Var(\hat z_p)\approx \nabla z_p^T \ V \ \nabla z_p$, donde V es la matriz de var-cov de los estimadores de cada parametro
>
>   Capaz se pueda graficar el nivel de retorno en funcion del umbral, Y su varianza, para ver como var칤a a medida que tenemos menos datos en valores m치s extremos.
>
>   Y como tengo tableta de dibujo, te hago un dibujito:
>
> ![varianza-del-nivel-de-retorno.png](../Imagenes/varianza-del-nivel-de-retorno.png)


```{r}

```

### Intuicion de distribuci칩n del m치ximo.

Sabemos que un histograma aproxima *(de una forma burda)* la funci칩n de densidad $f(x)$ a partir de la cual se generaron los datos $x_1, x_2, \dots, x_n$, asumiendo que son **realizaciones** de variables aleatorias $X_1, X_2, \dots, X_n \ \text{iid}$ con $X_1 \sim f$ 

```{r}
gridx <- seq(-3,3, 0.1)
hist(rnorm(1e5), prob=T, ylim=c(0,0.45), col='steelblue', main="Histograma de Datos ~ N(0,1)")
lines(gridx, dnorm(gridx), lwd=3, col='orange')
```

<!-- #region -->
En el problema de modelado de m치ximos, queremos encontrar una funci칩n $g(x)$ a partir de la cual se generaron los datos $\hat x_1, \hat x_2, \dots ,\hat x_m$, asumiendo que son **realizaciones** de variables aleatorias $\hat X_1, \hat X_2, \dots, \hat X_m \ \text{iid}$ con $\hat X_1 \sim g$

Siendo concretos, los datos que queremos modelar son:

* Los que sobrepasan alg칰n umbral

  * ej: "$x_i$ mayores a 30", "$x_i$ mayores al 95 percentil"
    
    
* Los m치ximos en un per칤odo (bloque) dado

  * ej: "teniendo $x_i$ diarios, quiero el mayor $x_i$ de cada a침o"
  
Llamamos $\hat x_i$ a los $x_i$ que cumplen su condici칩n de "**m치ximo**" en cualquiera de los casos.
<!-- #endregion -->

<!-- #region -->
En otras palabras, solo estamos interesados en **la cola de la distribuci칩n**.


### Antes de pasar al siguiente ejercicio:

>  쯇od칠s imaginar c칩mo resultar칤a un histograma de los **valores m치ximos** $\hat x_i$ que se encuentran **en la cola** de una Normal(0,1) como la anterior, a medida que la cantidad de muestras tiende a infinito?
>
> 쯇od칠s ver por qu칠 칠sto no es as칤? Qu칠 sucede "alrededor" del umbral? 쯇or qu칠?

> > ### RTA
> > Es razonable pensar que un histograma de los valores de la cola de una Normal, ser치 exactamente igual que recortar la cola de un histograma "completo".
> > 
> > Pero al tener un umbral que **descarta** los valores por debajo del mismo, podemos ver c칩mo la probabilidad (치rea) de caer en un entorno de $u$ se redujo a la mitad
> >
> > ![intro.png](../Imagenes/intro.png)
<!-- #endregion -->

# Ejercicio 1.

Realice un **histograma del percentil 90** como umbral de una muestra **Normal est치ndar** de `1e5` elementos.

Agregue una curva de densidad estimada $\hat g$ utilizando la funci칩n `density` sobre los **datos del histograma**.


### RTA

Realizamos un histograma de los datos por encima del umbral. Para filtrarlos:
> `datos[datos > umbral]`

```{r}
datos  <- rnorm(1e5)
umbral <- quantile(datos, 0.9, names=F)
datos_max <- datos[datos > umbral]
g_estim   <- density(datos_max, bw=0.1)
hist(datos_max, prob=T, col='steelblue', ylim=c(0, max(g_estim$y)))
lines(g_estim, lwd=3, col='orange')
```

```{r}

```

# Nivel de Retorno - GEV

> TODO: Plot de Ret level vs -log(1-p) en escala logaritmica pag 58


Por Teorema sabemos que la distribuci칩n de m치ximos tiende a una GEV cuando $n \rightarrow \infty$

$$\Large \mathrm P \left(\frac{M_n-b_n}{a_n} \leq z \right) \xrightarrow[]{n\rightarrow \infty} G(z) = \exp\left\{ - \left[ 1 + \xi \ \left( \frac{z-\mu}{\sigma} \right) \right]^{-1/\xi} \right\} $$

El nivel de retorno cuando $\xi = 0$ est치 dado por:

$$\Large z_p = \mu - \sigma \log\{-\log(1-p)\}$$

(pag 48-49, Coles & Brenner)

Y lo que queremos hacer es estimar sus par치metros y obtener un nivel de retorno estimado.

Vamos a avanzar suponiendo que $\xi = 0$, con lo que $G(z)$ es ahora de la **Familia Gumbel**

$$\Large G(z) = \exp \left\{ -\exp \left( -\frac{z-\mu}{\sigma} \right) \right\}$$

EJ: Con `gum.fit` encuentre el par치metro $\hat \sigma$ y $\hat \mu$

```{r}
#install.packages('evd')
#install.packages('ismev')
library(heatwaveR)
library(evd)
library(ismev)
```

Guarde los datos m치ximos diarios de la librer칤a `heatwaveR` y filtre del dataset `Algiers` los m치ximos de cada a침o (los datos de Algiers son diarios)

```{r}
#datos <- rgumbel(1e3)
datos <- Algiers[, 'tMax']
```

```{r}
# Me quedo con el max de cada a침o
n <- length(datos)
datos_max <- c()
i <- 1
for(d in seq(1, n, 365)){
    if( (n-365) < 0 ){
        # Ultimas mediciones
        datos_max[i] <- max(datos[d:n])
    }else{
        datos_max[i] <- max(datos[d:d+365-1])
    }
    i <- i + 1
}
```
1 . 

Explore el comando `gum.fit` de la librer칤a `ismev` usando los datos del ejercicio.

쯈u칠 valores se corresponden a las estimaciones de m치xima verosimilitud? 쯈u칠 representa `$se`?

```{r}
gum <- gum.fit(datos_max[-length(datos_max)])
gum$mle
```

        $conv
        [1] 0

        $nllh
        [1] 1561.445

        $mle
        [1] 0.03636916 0.98582371

        $se
        [1] 0.03283458 0.02429191



2 .

Reemplace los valores estimados en la funci칩n de nivel de retorno $z_p$

Defina una funci칩n `estim_zp` que tome como input el nivel $p$ y las estimaciones de $\mu$ y $\sigma$, y devuelva el nivel de retorno $z_p$

Compute el valor de zp para las estimaciones de 1, 

> **Podemos decir que el nivel de retorno $z_p$ es el valor que puede ser superado por el m치ximo anual en cualquier a침o, con probabilidad $p$**

```{r}
estim_zp <- function(p, mu, sigma){
    zp <- mu - sigma * log( -log(1-p) )
    return(zp)
}
```

3. Graficar $z_p$ en funci칩n de $\log(y_p)$ para una grilla de valores entre 0.01 y 0.99, usando los par치metros estimados en 1.

```{r}
mu    <- gum$mle[1]
sigma <- gum$mle[2]
```

```{r}
gridp <- seq(0.01, 0.99, 0.01)
m <- length(gridp)
estims <- rep(NA, m)
logyps <- rep(NA, m)
for(i in 1:m){
    p <- gridp[i]
    estims[i] <- estim_zp(p, mu, sigma)
    yp <- -log(1-p)
    logyps[i] <- log(yp)
}

```

```{r}
plot(estims, logyps, xlab=expression(z[p]), ylab=expression(log(y[p])), pch=20)
```

### Varianza del estimador

Por el [M칠todo Delta](https://es.wikipedia.org/wiki/M%C3%A9todo_delta), sabemos que: 

$$\Large \mathrm{Var}(\hat z_p)\approx \nabla z_p^T \ \mathbf V \ \nabla z_p$$

Donde

> $\large \nabla z_p^T = \left[ \frac{\partial z_p}{\partial \mu},\frac{\partial z_p}{\partial \sigma}  \right]$

Derivando el $z_p$ de arriba

> $\large \frac{\partial z_p}{\partial \mu} = 1$
>
> $\large \frac{\partial z_p}{\partial \sigma} = -\log\{ -\log (1-p)\}$


4 .

Repita lo mismo que 3 pero agregando la desviaci칩n est치ndar en el gr치fico (por encima y debajo de la curva).

Utilice la estimaci칩n de la matriz de covarianza `$cov` que devuelve `gum.fit` junto con los gradientes del estimador de $z_p$

TIP: Use el operador `%*%` para computar el producto entre matrices, `t()` para trasponer, y el comando `matrix(c(1, ds))` para generar una matriz de un array de datos.

```{r}
cov <- gum$cov

gridp <- seq(0.01, 0.99, 0.01)
m <- length(gridp)
zps <- rep(NA, m)
vars <- rep(NA, m)
logyps <- rep(NA, m)
for(i in 1:m){
    p <- gridp[i]
    du <- 1
    ds <- -log( -log(1-p) ) 
    grad_zp <- matrix( c(du, ds) )
    
    zps[i]  <- estim_zp(p, mu, sigma)
    vars[i] <- t(grad_zp) %*% cov %*% grad_zp 
    yp <- -log(1-p)
    logyps[i] <- log(yp)
}

```

```{r}
plot(logyps,  zps, xlab=expression(log(y[p])), ylab=expression(z[p]), pch=20)
lines(logyps, zps+vars)
lines(logyps, zps-vars)

plot(gridp, zps, xlab=expression(z[p]), ylab=expression(p), pch=20)
lines(gridp, zps+vars)
lines(gridp, zps-vars)

plot(gridp[70:90], zps[70:90], xlab=expression(z[p]), ylab=expression(p), pch=20)
lines(gridp[70:90], zps[70:90]+vars[70:90])
lines(gridp[70:90], zps[70:90]-vars[70:90])
```

---

Capaz la pareto dejarla como opcional con otra cosa o fuera.

TODO: Ver si el mismo analisis que antes devuelve m치s o menos lo mismo o hay cosas muy distintas a destacar.

<!-- #region -->
# Nivel de Retorno - Pareto


Es conveniente usar escalas anuales para calcular el nivel de retorno, de forma tener una interpretaci칩n simple:
> ***El nivel de retorno anual N, es el valor que se espera superar 1 sola vez cada N a침os***

Partiendo de que queremos calcular el 치rea a derecha de un umbral, de una funci칩n de densidad que aproximamos con una de la **Familia Pareto Generalizada**

$$\Large \mathrm P(X>x | X>u) = \left[ 1 + \xi \left( \frac{x-u}{\sigma} \right) \right]^{-1/\xi}$$

usando definici칩n de probabilidad condicional e igualando a $\frac 1 n$, obtenemos para el caso $\xi=0$

$$\Large z_n = u + \sigma \log (N \ n_y \ \mathrm P(X>u))$$

*(p치gina 81 en Coles & Trener)*
<!-- #endregion -->

EJ: Estimar el valor del par치metro $\sigma$ de la Pareto usando la funci칩n `ver funcion de R`
